{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "second-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "executive-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/exp4/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "saved-parish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hook]\n",
      "I've been down so long, it look like up to me\n",
      "They look up to me\n",
      "I got fake people showin' fake love to me\n",
      "Straight up to my face, straight up to my face\n",
      "I've been down so long, it look like up to me\n",
      "They look up to me\n",
      "I got fake people showin' fake love to me\n",
      "Straight up to my face, straight up to my face [Verse 1]\n",
      "Somethin' ain't right when we talkin'\n",
      "Somethin' ain't right when we talkin'\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "\n",
    "    if idx > 10: break    \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-cancer",
   "metadata": {},
   "source": [
    "## 중괄호[ ]로 절과 후렴을 표시하므로 소문자로 정리한 뒤 해당 요소를 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "affected-expression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(\"\\[.*\\]|\\s-\\s.*\", \"\", sentence)\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This [Hook] [Verse 1 : Drake] @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-madness",
   "metadata": {},
   "source": [
    "## 띄어쓰기를 세는 방식으로 start와 end를 포함하여 토큰이 15개 이하인 문장만 corpus에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "composite-league",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>  <end>',\n",
       " '<start> i ve been down so long , it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin fake love to me <end>',\n",
       " '<start> straight up to my face , straight up to my face <end>',\n",
       " '<start> i ve been down so long , it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin fake love to me <end>',\n",
       " '<start> straight up to my face , straight up to my face <end>',\n",
       " '<start> somethin ain t right when we talkin <end>']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    count = 0\n",
    "    for i in preprocessed_sentence:\n",
    "        if i == \" \":\n",
    "            count += 1\n",
    "    if count < 15:\n",
    "        corpus.append(preprocessed_sentence)\n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "clinical-thirty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of spaces  4\n"
     ]
    }
   ],
   "source": [
    "def check_space(corpus):\n",
    "   \n",
    "    c = 0\n",
    "        \n",
    "    for i in range(0, len(corpus)):\n",
    "        if corpus[i] == \" \":\n",
    "            c +=1\n",
    "    print(\"number of spaces \", c)\n",
    "              \n",
    "\n",
    "check_space(corpus[350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "instructional-sugar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   3   0 ...   0   0   0]\n",
      " [  2   4  95 ...  10  12   3]\n",
      " [  2  38 133 ...   0   0   0]\n",
      " ...\n",
      " [  2   3   0 ...   0   0   0]\n",
      " [  2   3   0 ...   0   0   0]\n",
      " [  2   3   0 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f558942ce90>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=12000, filters=' ',oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')      \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "indian-brain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167475, 15)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "rotary-equivalent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   4  95 105  58  31 167   5  11 133  23  29  10  12]\n",
      "[  4  95 105  58  31 167   5  11 133  23  29  10  12   3]\n"
     ]
    }
   ],
   "source": [
    "enc_input = tensor[:,:-1]\n",
    "dec_input = tensor[:,1:]\n",
    "\n",
    "print(enc_input[1])\n",
    "print(dec_input[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-buffalo",
   "metadata": {},
   "source": [
    "## 8:2로 train 데이터와 validation 데이터를 분리했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "certified-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(enc_input, dec_input, test_size = 0.2, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "alert-hindu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (133980, 14)\n",
      "Target Train: (133980, 14)\n",
      "Source Val: (33495, 14)\n",
      "Target Val: (33495, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(\"Source Val:\", enc_val.shape)\n",
    "print(\"Target Val:\", dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "extended-macro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 14), (64, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset_train = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "sunrise-animal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 14), (64, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_val)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(enc_val) // BATCH_SIZE\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset_val = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-affect",
   "metadata": {},
   "source": [
    "## LSTM을 기본으로 하이퍼파라미터튜닝으로는 과제에서 주어진 목표 val_loss에 근접했으나 2.2를 하회하는 결과값을 얻을 수 없었습니다.\n",
    "## bidirectional LSTM과 Dropout등을 활용하여 모델을 만들어보았지만 val_loss는 획기적으로 줄여도 정작 모델 출력이 되지 않는 현상이 있었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "suited-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "mature-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2093/2093 [==============================] - 272s 128ms/step - loss: 3.2360 - val_loss: 2.6757\n",
      "Epoch 2/10\n",
      "2093/2093 [==============================] - 267s 128ms/step - loss: 2.5608 - val_loss: 2.4976\n",
      "Epoch 3/10\n",
      "2093/2093 [==============================] - 267s 128ms/step - loss: 2.3232 - val_loss: 2.3856\n",
      "Epoch 4/10\n",
      "2093/2093 [==============================] - 269s 128ms/step - loss: 2.1079 - val_loss: 2.3053\n",
      "Epoch 5/10\n",
      "2093/2093 [==============================] - 264s 126ms/step - loss: 1.9138 - val_loss: 2.2589\n",
      "Epoch 6/10\n",
      "2093/2093 [==============================] - 266s 127ms/step - loss: 1.7347 - val_loss: 2.2221\n",
      "Epoch 7/10\n",
      "2093/2093 [==============================] - 265s 126ms/step - loss: 1.5775 - val_loss: 2.2102\n",
      "Epoch 8/10\n",
      "2093/2093 [==============================] - 266s 127ms/step - loss: 1.4418 - val_loss: 2.2063\n",
      "Epoch 9/10\n",
      "2093/2093 [==============================] - 266s 127ms/step - loss: 1.3252 - val_loss: 2.2124\n",
      "Epoch 10/10\n",
      "2093/2093 [==============================] - 265s 127ms/step - loss: 1.2207 - val_loss: 2.2291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f54021aa310>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset_train, epochs=10, validation_data=dataset_val, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-greensboro",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "surrounded-placement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_73 (Embedding)     multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm_142 (LSTM)              multiple                  6295552   \n",
      "_________________________________________________________________\n",
      "lstm_143 (LSTM)              multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 33,133,793\n",
      "Trainable params: 33,133,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "secondary-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-sword",
   "metadata": {},
   "source": [
    "## 만족스러운 첫 작사 결과물"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sonic-services",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you more than i love myself <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-sister",
   "metadata": {},
   "source": [
    "## 두번째 모델 작사 결과물"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "minus-president",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-patrick",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
