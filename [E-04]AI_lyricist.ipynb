{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elder-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "absolute-light",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/exp4/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "illegal-target",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hook]\n",
      "I've been down so long, it look like up to me\n",
      "They look up to me\n",
      "I got fake people showin' fake love to me\n",
      "Straight up to my face, straight up to my face\n",
      "I've been down so long, it look like up to me\n",
      "They look up to me\n",
      "I got fake people showin' fake love to me\n",
      "Straight up to my face, straight up to my face [Verse 1]\n",
      "Somethin' ain't right when we talkin'\n",
      "Somethin' ain't right when we talkin'\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "\n",
    "    if idx > 10: break    \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-southeast",
   "metadata": {},
   "source": [
    "## 중괄호[ ]로 절과 후렴을 표시하므로 소문자로 정리한 뒤 해당 요소를 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "federal-involvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(\"\\[.*\\]|\\s-\\s.*\", \"\", sentence)\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This [Hook] [Verse 1 : Drake] @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-planet",
   "metadata": {},
   "source": [
    "## 띄어쓰기를 세는 방식으로 start와 end를 포함하여 토큰이 15개 이하인 문장만 corpus에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "boring-retrieval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> i swear to god <end>',\n",
       " '<start> promise of a better world to come <end>',\n",
       " '<start> for the childhood <end>',\n",
       " '<start> and learn to use my toys <end>',\n",
       " '<start> from the bottom so the tops <end>',\n",
       " '<start> heard muthafuckers talk , seen em drop <end>',\n",
       " '<start> sister confused , she went alone <end>',\n",
       " '<start> sometimes possibly maybe probably love <end>',\n",
       " '<start> so i ain t got a thing to lose . <end>',\n",
       " '<start> and went his way . <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_a = []\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    count = 0\n",
    "    for i in preprocessed_sentence:\n",
    "        if i == \" \":\n",
    "            count += 1\n",
    "    if count < 15:\n",
    "        corpus_a.append(preprocessed_sentence)\n",
    "\n",
    "corpus = list(set(corpus_a))\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "composite-extent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of spaces  14\n"
     ]
    }
   ],
   "source": [
    "def check_space(corpus):\n",
    "   \n",
    "    c = 0\n",
    "        \n",
    "    for i in range(0, len(corpus)):\n",
    "        if corpus[i] == \" \":\n",
    "            c +=1\n",
    "    print(\"number of spaces \", c)\n",
    "              \n",
    "\n",
    "check_space(corpus[350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "severe-volunteer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    5  516 ...    0    0    0]\n",
      " [   2  633   18 ...    0    0    0]\n",
      " [   2   28    6 ...    0    0    0]\n",
      " ...\n",
      " [   2 7746   32 ...    0    0    0]\n",
      " [   2  121   26 ...   44    3    0]\n",
      " [   2 6002 8150 ... 6210    3    0]] <keras_preprocessing.text.Tokenizer object at 0x7fe46c6f7590>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=12000, filters=' ',oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')      \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "killing-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 633  18   9 159 141  10  75   3   0   0   0   0   0]\n",
      "[633  18   9 159 141  10  75   3   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "enc_input = tensor[:,:-1]\n",
    "dec_input = tensor[:,1:]\n",
    "\n",
    "print(enc_input[1])\n",
    "print(dec_input[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-engineer",
   "metadata": {},
   "source": [
    "## 노드 예제에는 따로 트레이닝 데이터와 테스트 데이터 분할을 하지 않았는데 과제에는 하라고 나와서 진행하긴 했습니다만 테스트 데이터가 필요한 과제인지 모르겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "medical-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_test, dec_train, dec_test = train_test_split(enc_input, dec_input, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "square-turtle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (80567, 14)\n",
      "Target Train: (80567, 14)\n",
      "Source Test: (20142, 14)\n",
      "Target Test: (20142, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(\"Source Test:\", enc_test.shape)\n",
    "print(\"Target Test:\", dec_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-sellers",
   "metadata": {},
   "source": [
    "## 10epoch의 학습으로 좋은 결과물을 만들어야하는 만큼 배치사이즈를 줄이는 것이 합당해 보입니다.\n",
    "\n",
    "## 배치 사이즈를 64로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "remarkable-attitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 14), (64, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset_train = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collected-surface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 14), (64, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_test)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(enc_test) // BATCH_SIZE\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_test, dec_test))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset_test = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-holocaust",
   "metadata": {},
   "source": [
    "## 입력층 수 512, LSTM의 노드 수를 1024로 설정했습니다.\n",
    "## LSTM을 여러겹 사용할 경우 각 층의 노드 숫자를 통일하는 것이 전반적으로 성능이 좋다고 나와서 일단은 두 LSTM층 모두 1024개의 노드로 유지 했습니다.\n",
    "## 임베딩 사이즈를 키우는 것이 자연어처리에서의 섬세함과 연관된다고 하지만 은닉층의 노드보다 큰 숫자는 의미가 없기에 적절하게 512정도로 늘리기로 결정했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "behavioral-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "textile-digest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1258/1258 [==============================] - 154s 120ms/step - loss: 2.2817\n",
      "Epoch 2/10\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 2.1348\n",
      "Epoch 3/10\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 2.0093\n",
      "Epoch 4/10\n",
      "1258/1258 [==============================] - 150s 119ms/step - loss: 1.8915\n",
      "Epoch 5/10\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 1.7909\n",
      "Epoch 6/10\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 1.6841\n",
      "Epoch 7/10\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 1.5809\n",
      "Epoch 8/10\n",
      "1258/1258 [==============================] - 153s 122ms/step - loss: 1.4891\n",
      "Epoch 9/10\n",
      "1258/1258 [==============================] - 155s 123ms/step - loss: 1.4017\n",
      "Epoch 10/10\n",
      "1258/1258 [==============================] - 195s 155ms/step - loss: 1.3199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe46c322210>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-repository",
   "metadata": {},
   "source": [
    "## 운이 좋게도 첫 학습에서 로스가 1.31로 나왔습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "advanced-level",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  6295552   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 33,133,793\n",
      "Trainable params: 33,133,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "manual-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "southeast-binary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you more than i love myself <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-nelson",
   "metadata": {},
   "source": [
    "## 만족스러운 첫 작사 결과물"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-flavor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
