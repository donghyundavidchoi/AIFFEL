{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "announced-newman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compressed-costume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import seaborn\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-character",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-andorra",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "weekly-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/nlp10/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-adoption",
   "metadata": {},
   "source": [
    "# zip과 set을 활용하여 순서쌍을 유지하면서 중복 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "structured-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    cleaned_corpus = list(set(zip(kor,eng)))\n",
    "\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rational-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advance-adams",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78968"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-corpus",
   "metadata": {},
   "source": [
    "# 전처리 및 센텐스피스를 사용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "residential-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^a-zㄱ-ㅎ가-힣?.!,]+\", \" \", sentence)\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "surgical-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer(corpus, vocab_size, lang=\"ko\", pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "    \n",
    "    temp_file = os.getenv('HOME') + f'/aiffel/nlp10/transformer/corpus_{lang}.txt'\n",
    "    \n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "    \n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={temp_file} --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} \\\n",
    "        --unk_id={unk_id} --model_prefix=spm_{lang} --vocab_size={vocab_size}'\n",
    "    )\n",
    "    \n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f'spm_{lang}.model')\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sacred-issue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair[0], pair[1]\n",
    "\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "antique-meaning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf5af89f8e54e59afe2b74597849bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    src = ko_tokenizer.EncodeAsIds(kor_corpus[idx])\n",
    "    tgt = en_tokenizer.EncodeAsIds(eng_corpus[idx])\n",
    "    \n",
    "    if len(src) <= 50 and len(tgt) <= 50:\n",
    "        src_corpus.append(src)\n",
    "        tgt_corpus.append(tgt)\n",
    "\n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-alias",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-recall",
   "metadata": {},
   "source": [
    "# Transformer 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "disciplinary-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "furnished-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        \n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)       \n",
    "        QK = tf.matmul(Q,K,transpose_b=True)\n",
    "        scaled_qk = QK/tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9) \n",
    "        \n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "    \n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        \n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(WQ_splits, WK_splits, WV_splits, mask)\n",
    "        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "compact-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        \n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "radio-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "coastal-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "controversial-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "addressed-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ruled-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, pos_len, dropout=0.2, shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        \n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "informal-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-myanmar",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-universe",
   "metadata": {},
   "source": [
    "# 러닝레이트 및 옵티마이저 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "located-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "political-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "clinical-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-digest",
   "metadata": {},
   "source": [
    "# loss 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "behind-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-fetish",
   "metadata": {},
   "source": [
    "# train step 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "surface-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-harvest",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-puppy",
   "metadata": {},
   "source": [
    "# 학습 및 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "maritime-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(n_layers=2, d_model=512, n_heads=8, d_ff = 2048, src_vocab_size=SRC_VOCAB_SIZE, tgt_vocab_size=TGT_VOCAB_SIZE, pos_len=200, dropout=0.1, shared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "spare-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "posted-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens], maxlen=enc_train.shape[-1], padding='post')\n",
    "    \n",
    "    print(len(_input))\n",
    "    print(enc_train.shape[-1])\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "        \n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(_input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        \n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "packed-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "flush-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "nasty-harbor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7dbe52baa624209a7991ab349bbd788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s obama is a campaign .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they are being in the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it is no longer .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the attacks of the death toll in the city of the city .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1253a059f9d742b299e161ab679c3ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a president .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is around the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee don t think you don t think you don t think you don t think you don t think you don t think .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: thousands of dead were dead .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c2d95165384baca1196921160d5bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city s city is a city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee needs to be a coffee .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people dead and a dead .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438f1f04e2b84ec0ab0499d9f91ca460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city s city mountain in the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee is no need for coffee .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: death toll from the death toll monday .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673e21f50f4b47f392364185699ca1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a presumptive president .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they re the city in town of the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there need no need to be no need to keep any of anyone needs to be needed .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: two of the dead were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c892990aaed481ab9eee0fde12a27ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president obama is live with .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city san cha commemorates san city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no otherwise .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven death toll from the ministry of seven consecutive days monday .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47989f0052064c4baf6474efa935a3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: he is the president .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city s mann in the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there s no need to win anywhere .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven deaths were confirmed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9515701a9a34409fb17670a6baceebbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: citizens thought the city down sanion in the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no need to keep casualties .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were dead thursday .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd4165f105a4eee985e276ffe3fe45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is then .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city authorities are outside the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there needs to be no rollless .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed , officials said monday .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffaaeb91f4444129e6b9f562cc10ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is taking the presidential election .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city streets take the city down city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no need for even .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven other people were dead , .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cb535e0e33486f88e3564a0deb87c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is taking the president s presidential election .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is only san .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no need to need to coffee .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven others were killed , thursday s dead .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7c7ab981ca4577aae1012c32948913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is also live just becoming the shortest .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: citizens are going to control over city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there need no work need to be coffee .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed when the deaths occurred .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0106891b6c84866a819a4734f02326d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: that is obama .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city take the city on saturday .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the need for coffee .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed , officials said .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a45a800a024272842642841860b229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president obama is live at the president .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city took place on the city streets in san city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no need to need for start .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: three people died and seven other dead .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab97febab8e044e3b53fd48034e13c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is taking the presidential nominee .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city took control of the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no need to need to need coffee .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: another seven were wounded .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af9c2ba0a1d4f3c9dd18aa4339eb3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is then .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city took some of the cityy san city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: for no need have to been needed .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people died on thursday , .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d1a318cbfc4ff983e6821e7a9249cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: citizens took some the city to control the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: at no need to need to need to keep it .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were dead were seven .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7735cdcbfb1488db240f4f715317dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: that is president obama .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city took eight the city in the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: at no need is needed .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: another people died because , another seven people were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bc46ec02414f6a8fef153ef4a9a350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: he is president a short story that obama is a short african country .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city took some in city to the city streets in the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: for no need to buy it .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were dead in the dead .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ba32e2d8e94840ab599a175a3b056a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a president .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city took some of sanish streets in the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: at no coffee , he said .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven elderly people have died in that thursday s death .\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-microwave",
   "metadata": {},
   "source": [
    "# 고찰\n",
    "\n",
    "학습과정에서 loss는 계속 낮아졌지만 학습 과정에서 번역 성능은 들쑥날쑥하는 것을 볼 수 있었습니다.  \n",
    "transformer를 적용하긴 했지만 아직 성능 개선의 여지가 많이 남아 있는 모습입니다. \n",
    "transformer의 성능 자체가 아쉬움이 많은 것인지, 작업 환경의 제약 때문에 논문에 나와 있는 수준으로 하이퍼파라미터를 튜닝할 수 없어서 생긴 성능의 아쉬움인지는 잘 모르겠습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-passport",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "english-injection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 러시아의 우크라이나 침공이 가시화되고 있습니다.\n",
      "Predicted translation: russia s closest smoothly climbed into situations .\n"
     ]
    }
   ],
   "source": [
    "translate(\"러시아의 우크라이나 침공이 가시화되고 있습니다.\", transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "measured-interim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 미국의 핵잠수함 제공에 대해 프랑스는 불편한 내색을 감추지 않고 있습니다.\n",
      "Predicted translation: you know about your enemy ice , maybe i ask you to study your favorite bills .\n"
     ]
    }
   ],
   "source": [
    "translate(\"미국의 핵잠수함 제공에 대해 프랑스는 불편한 내색을 감추지 않고 있습니다.\", transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "younger-layout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 유럽의 천연가스 가격이 급등하고 있습니다.\n",
      "Predicted translation: european gas prices are being priceed on rise .\n"
     ]
    }
   ],
   "source": [
    "translate(\"유럽의 천연가스 가격이 급등하고 있습니다.\", transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "revised-cement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 전세계적으로 공급망 이슈는 지속될 것으로 보입니다.\n",
      "Predicted translation: another issue that just will be issued .\n"
     ]
    }
   ],
   "source": [
    "translate(\"전세계적으로 공급망 이슈는 지속될 것으로 보입니다.\", transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-variance",
   "metadata": {},
   "source": [
    "## 이전 seq2seq 과제에 비해서 좀 더 그럴싸한 번역을 보여줍니다.\n",
    "## 다만 데이터에 없어보일 법한 문장에 대해서는 성능이 떨어지는 듯해보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-brunei",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
