{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "entertaining-capacity",
   "metadata": {},
   "source": [
    "# 필요한 패키지를 설치하고 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "athletic-donor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mecab-ko is already installed\n",
      "mecab-ko-dic is already installed\n",
      "mecab-python is already installed\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/v0.6.0/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "silent-million",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.3 in /opt/conda/lib/python3.7/site-packages (3.8.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim==3.8.3) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim==3.8.3) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim==3.8.3) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim==3.8.3) (4.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suitable-palestine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from konlpy.tag import Mecab\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-davis",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-consumer",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "occupational-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = os.getenv('HOME') + '/aiffel/nlp12/chatbot/data/ChatbotData.csv'\n",
    "data = pd.read_csv(data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tribal-county",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cognitive-issue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 열에서 중복을 배제한 유일한 샘플의 수 : 11662\n",
      "A 열에서 중복을 배제한 유일한 샘플의 수 : 7779\n"
     ]
    }
   ],
   "source": [
    "print('Q 열에서 중복을 배제한 유일한 샘플의 수 :', data['Q'].nunique())\n",
    "print('A 열에서 중복을 배제한 유일한 샘플의 수 :', data['A'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wanted-anatomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 7731\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(subset = ['Q'], inplace=True)\n",
    "data.drop_duplicates(subset = ['A'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "favorite-arizona",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['12시 땡!', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', ..., '훔쳐보는 것도 눈치 보임.',\n",
       "       '흑기사 해주는 짝남.', '힘든 연애 좋은 연애라는게 무슨 차이일까?'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Q'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-contest",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-certification",
   "metadata": {},
   "source": [
    "# 질문 데이터와 답변 데이터로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dense-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "que = data['Q'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "third-japan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ans = data['A'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-jumping",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-thickness",
   "metadata": {},
   "source": [
    "# 전처리와 Mecab을 통한 토큰화를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "constant-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sunset-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^0-9a-zㄱ-ㅎ가-힣?.!,]+\", \" \", sentence)\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = mecab.morphs(sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "backed-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus():\n",
    "    \n",
    "    que_corpus, ans_corpus = [], []\n",
    "    \n",
    "    for i in range(len(que)):\n",
    "        if 2 <= len(que[i]) <= 40 and 2 <= len(ans[i]) <= 40:\n",
    "            que_corpus.append(preprocess_sentence(que[i]))\n",
    "            ans_corpus.append(preprocess_sentence(ans[i]))\n",
    "    \n",
    "    return que_corpus, ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "literary-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus, ans_corpus = build_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "harmful-embassy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7649"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(que_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adaptive-butterfly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7649"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ans_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-shareware",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-barbados",
   "metadata": {},
   "source": [
    "# ko.bin으로 word2vec을 생성하고 lexical_sub 함수로 데이터 augmentation을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fourth-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_path = os.getenv('HOME') + '/aiffel/nlp12/chatbot/data/ko.bin'\n",
    "wv = gensim.models.Word2Vec.load(ko_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "english-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "    \n",
    "    res = \"\"\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "        \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "significant-consumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7649 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n",
      "100%|██████████| 7649/7649 [00:28<00:00, 264.31it/s]\n",
      "100%|██████████| 13451/13451 [00:51<00:00, 262.02it/s]\n"
     ]
    }
   ],
   "source": [
    "que_corpus_aug = que_corpus\n",
    "ans_corpus_aug = ans_corpus\n",
    "AUG_TIMES = 3\n",
    "\n",
    "for mul in range(AUG_TIMES - 1):\n",
    "    for i in tqdm(range(len(que_corpus))):\n",
    "        que_corpus_new = lexical_sub(que_corpus[i], wv)\n",
    "        ans_corpus_new = lexical_sub(ans_corpus[i], wv)\n",
    "\n",
    "        if que_corpus_new is not None and ans_corpus_new is not None:\n",
    "            que_corpus_aug.append(preprocess_sentence(que_corpus_new))\n",
    "            ans_corpus_aug.append(preprocess_sentence(ans_corpus_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dress-impression",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23973"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(que_corpus_aug)\n",
    "len(ans_corpus_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-technician",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-subject",
   "metadata": {},
   "source": [
    "# 백터화를 진행합니다.\n",
    "## 답변 데이터에 시작 토큰과 종료 토큰을 추가하고 최대길이에 맞춰 패딩을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eight-december",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(que_corpus_aug, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "excessive-expansion",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(ans_corpus_aug, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "identical-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ans_corpus_aug)):\n",
    "    ans_corpus_aug[i] = [\"<start>\"] + ans_corpus_aug[i] + [\"<end>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "intellectual-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 27\n",
    "\n",
    "def tokenize(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000, filters=' ',oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(inputs + outputs)\n",
    "    \n",
    "    tokenized_inputs = tokenizer.texts_to_sequences(inputs)   \n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, padding='post', maxlen=MAX_LENGTH)      \n",
    "    \n",
    "    tokenized_outputs = tokenizer.texts_to_sequences(outputs)   \n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, padding='post', maxlen=MAX_LENGTH)      \n",
    "    \n",
    "    \n",
    "    return tokenized_inputs, tokenized_outputs, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "metallic-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, dec_train, tokenizer = tokenize(que_corpus_aug, ans_corpus_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "significant-origin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x7fe449483390>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "identified-billion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7212"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "designing-gambling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7531"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "standard-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = [tokenizer.word_index[\"<start>\"]]\n",
    "END_TOKEN = [tokenizer.word_index[\"<end>\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "environmental-venture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "print(START_TOKEN)\n",
    "print(END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "completed-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 7528"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-hormone",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-struggle",
   "metadata": {},
   "source": [
    "# 앞선 노드에서 구현한 트랜스포머를 다시 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "laughing-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "killing-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        \n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)       \n",
    "        QK = tf.matmul(Q,K,transpose_b=True)\n",
    "        scaled_qk = QK/tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9) \n",
    "        \n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "    \n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        \n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(WQ_splits, WK_splits, WV_splits, mask)\n",
    "        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "analyzed-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        \n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sapphire-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "numerical-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "annual-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "sharing-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "plain-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, pos_len, dropout=0.2, shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        \n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "resident-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "preliminary-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "embedded-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "scenic-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "jewish-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "informational-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-fundamentals",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-humanitarian",
   "metadata": {},
   "source": [
    "# 이번 노드는 데이터가 적었던 만큼 과적합을 피하기 위해 하이퍼파라미터를 다시 튜닝하고 학습을 진행했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "designing-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(n_layers=2, d_model=512, n_heads=8, d_ff = 1024, src_vocab_size=20000, tgt_vocab_size=20000, pos_len=200, dropout=0.5, shared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "separated-south",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 375/375 [02:10<00:00,  2.88it/s, Loss 5.9505]\n",
      "Epoch  2: 100%|██████████| 375/375 [02:03<00:00,  3.04it/s, Loss 3.6896]\n",
      "Epoch  3: 100%|██████████| 375/375 [02:01<00:00,  3.08it/s, Loss 2.5659]\n",
      "Epoch  4: 100%|██████████| 375/375 [02:03<00:00,  3.04it/s, Loss 1.4888]\n",
      "Epoch  5: 100%|██████████| 375/375 [02:02<00:00,  3.06it/s, Loss 0.9651]\n",
      "Epoch  6: 100%|██████████| 375/375 [02:03<00:00,  3.05it/s, Loss 0.7745]\n",
      "Epoch  7: 100%|██████████| 375/375 [02:03<00:00,  3.03it/s, Loss 0.7192]\n",
      "Epoch  8: 100%|██████████| 375/375 [02:03<00:00,  3.05it/s, Loss 0.6985]\n",
      "Epoch  9: 100%|██████████| 375/375 [02:01<00:00,  3.08it/s, Loss 0.6886]\n",
      "Epoch 10: 100%|██████████| 375/375 [02:01<00:00,  3.07it/s, Loss 0.6890]\n",
      "Epoch 11: 100%|██████████| 375/375 [02:01<00:00,  3.07it/s, Loss 0.6964]\n",
      "Epoch 12: 100%|██████████| 375/375 [02:02<00:00,  3.07it/s, Loss 0.6352]\n",
      "Epoch 13: 100%|██████████| 375/375 [02:03<00:00,  3.03it/s, Loss 0.5500]\n",
      "Epoch 14: 100%|██████████| 375/375 [02:03<00:00,  3.05it/s, Loss 0.4874]\n",
      "Epoch 15: 100%|██████████| 375/375 [02:01<00:00,  3.08it/s, Loss 0.4396]\n",
      "Epoch 16: 100%|██████████| 375/375 [02:02<00:00,  3.05it/s, Loss 0.3960]\n",
      "Epoch 17: 100%|██████████| 375/375 [02:03<00:00,  3.04it/s, Loss 0.3554]\n",
      "Epoch 18: 100%|██████████| 375/375 [02:02<00:00,  3.06it/s, Loss 0.3244]\n",
      "Epoch 19: 100%|██████████| 375/375 [02:01<00:00,  3.08it/s, Loss 0.2997]\n",
      "Epoch 20: 100%|██████████| 375/375 [02:02<00:00,  3.07it/s, Loss 0.2752]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-leisure",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-accident",
   "metadata": {},
   "source": [
    "# 챗봇 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "subjective-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, tokenizer):\n",
    "    pieces = preprocess_sentence(sentence)\n",
    "    tokens = tokenizer.texts_to_sequences([pieces])\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences(tokens, maxlen=enc_train.shape[-1], padding='post')\n",
    "    ids = []\n",
    "    output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "        \n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(_input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        \n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        \n",
    "        if END_TOKEN == [predicted_id]:\n",
    "            result = tokenizer.sequences_to_texts([ids])\n",
    "            result = \"\".join(result)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    result = result\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "loose-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(sentence, model=transformer, tokenizer=tokenizer):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, tokenizer)\n",
    "    \n",
    "    print('질문: %s' % (sentence))\n",
    "    print('답변: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "meaningful-bench",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 밥은 먹고 다닐까?\n",
      "답변: 정신 이 힘든 건지 연락 해 보 세요 .\n"
     ]
    }
   ],
   "source": [
    "chatbot(\"밥은 먹고 다닐까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "russian-receptor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 안녕 좋은 아침이야\n",
      "답변: 잘 지내 고 좋 죠 .\n"
     ]
    }
   ],
   "source": [
    "chatbot(\"안녕 좋은 아침이야\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "informative-fourth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 맛있는 거 먹자\n",
      "답변: 맛있 는 거 드세요 .\n"
     ]
    }
   ],
   "source": [
    "chatbot(\"맛있는 거 먹자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "threaded-rocket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 설날이 다가오고 있어\n",
      "답변: 날려 버리 시 길 바랍니다 .\n"
     ]
    }
   ],
   "source": [
    "chatbot(\"설날이 다가오고 있어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "desirable-efficiency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 어제 안 되던 게 오늘은 왜 잘 되는걸까?\n",
      "답변: 달라지 는 건 없 다고 생각 해 보 세요 .\n"
     ]
    }
   ],
   "source": [
    "chatbot(\"어제 안 되던 게 오늘은 왜 잘 되는걸까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "excited-crash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 방학 한 주만 더 있었으면 좋겠다\n",
      "답변: 학생 이 아니 면 썸 이 죠 .\n"
     ]
    }
   ],
   "source": [
    "chatbot(\"방학 한 주만 더 있었으면 좋겠다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "polyphonic-thickness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 날씨가 쌀쌀하니 춥다\n",
      "답변: 하늘 을 보 고 받아들이 세요 .\n"
     ]
    }
   ],
   "source": [
    "chatbot(\"날씨가 쌀쌀하니 춥다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "formed-wrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 여행 가고 싶다\n",
      "답변: 계획 을 세워 보 세요 .\n"
     ]
    }
   ],
   "source": [
    "chatbot(\"여행 가고 싶다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "metric-narrative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 부모님 선물은 뭐가 좋을까?\n",
      "답변: 사랑 과 현금 이 면 충분 해요 .\n"
     ]
    }
   ],
   "source": [
    "chatbot(\"부모님 선물은 뭐가 좋을까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "hundred-uganda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 사랑했던 기억은 흐려지기만하는구나\n",
      "답변: 사랑 은 다시 만날 수 있 는 거 예요 .\n"
     ]
    }
   ],
   "source": [
    "chatbot(\"사랑했던 기억은 흐려지기만하는구나\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-private",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-london",
   "metadata": {},
   "source": [
    "# 고찰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-graph",
   "metadata": {},
   "source": [
    "패키지 관리부터 이슈가 많았던 노드였습니다.  \n",
    "처음에는 10번 노드와 똑같이 트랜스포머를 활용하는 노드라고 해서 쉽게 끝날 줄 알았습니다.  \n",
    "똑같이 트랜스포머를 사용하였고 거의 같은 코드를 공유했기에 그렇게 생각했으나 현실은 달랐습니다.  \n",
    "노드의 목표가 다른 만큼 전처리와 토큰화, 백터화에서 다른 방법을 사용해야했고 챗봇 구현 과정에서 이에 맞춰서 코드를 수정해줬어야했습니다.  \n",
    "이번 노드를 통해서 같은 딥러닝 이론을 활용하는 task라도 사용하는 데이터와 목적에 맞춰서 전처리와 토큰화, 백터화, 그리고 출력의 코드를 적절히 수정해줘야한다는 것을 배울 수 있었습니다.  \n",
    "특히나 애를 먹은 부분은 분명이 사용하는 패키지가 달라지는 것 뿐인데도 불구하고 각각의 패키지가 제공하는 매소드가 다르고 비슷한 매소드라도 입출력이 다른 만큼 함수에 따라 생각보다 디테일하게 수정을 진행했어야하는 점입니다.  \n",
    "이 과정에서 오류가 나는 부분에서 print 함수로 일일히 출력을 진행하면서 함수가 작동할 수 있도록 디버깅하는 것에 많은 시간을 투입하였습니다.  \n",
    "이번 12번 노드의 경우 exploration 15번에서도 진행했던 task였는데 그때 진행했던 방식과 비교를 해보는 것도 나름 재미있는 과정이었습니다.  \n",
    "전체적으로 챗봇의 결과물은 좀 더 심오해진 느낌을 받았습니다.  \n",
    "그러나 여전히 특정 단어의 존재 유무로 답변의 형태가 결정되거나, 학습 데이터에 없을 법한 단어들에 대해서는 챗봇이 제대로 답변하지 못 하는 문제는 여전함을 볼 수 있었습니다.  \n",
    "아마 위에서 언급한 문제들은 다음 노드에서 사용하게 될 BERT를 통해 어느 정도 해결 가능하리라 생각됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-court",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
